# Welcome to the NLP Specialization
![[index.mp4]]
# Welcome to Course 1
![[index_2.mp4]]
# Acknowledgement - Ken Church

We would like to thank Ken Church for lending his expertise to review the outline and topics of this specialization.

[Ken Church](https://scholar.google.com/citations?user=E6aqGvYAAAAJ) is currently a Distinguished Scientist at Baidu, and formerly led computational linguistics research at IBM, Microsoft, AT&T and Johns Hopkins University.  He was the president of the Association for Computational Linguistics (ACL) in 2012, and also president of the Special Interest Group on Linguistic Data & Corpus-based Approaches to Natural Language Processing (SIGDAT) from 1993 until 2011.

Thanks Ken for sharing your insights on what learners should know about natural language processing!

![[Pasted image 20240109013326.png]]

# Week Introduction
![[index_3.mp4]]

# Supervised ML & Sentiment Analysis
![[index_4.mp4]]
![[Pasted image 20240109013515.png]]

# Supervised ML & Sentiment Analysis
In supervised machine learning, you usually have an input $X$, which goes into your prediction function to get your $Y$. You can then compare your prediction with the true value $Y$ . This gives you your cost which you use to update the parameters $θ$. The following image, summarizes the process.

![[Pasted image 20240109013614.png]]
To perform sentiment analysis on a tweet, you first have to represent the text (i.e. "I am happy because I am learning NLP ") as features, you then train your logistic regression classifier, and then you can use it to classify the text.

![[Pasted image 20240109013631.png]]

Note that in this case, you either classify 1, for a positive sentiment, or 0, for a negative sentiment.

# Vocabulary & Feature Extraction

![[index_5.mp4]]

![[Pasted image 20240112233202.png]]

# Vocabulary & Feature Extraction

Given a tweet, or some text, you can represent it as a vector of dimension $V$, where $V$ corresponds to your vocabulary size. If you had the tweet "I am happy because I am learning NLP", then you would put a $1$ in the corresponding index for any word in the tweet, and a $0$ otherwise.

![[Pasted image 20240112233757.png]]

As you can see, as $V$ gets larger, the vector becomes more sparse. Furthermore, we end up having many more features and end up training $θ$ $V$ parameters. This could result in larger training time, and large prediction time.

# Negative and Positive Frequencies

![[index_6.mp4]]

# Feature Extraction with Frequencies

![[index_7.mp4]]

Given the following tweet and frequencies , what is the value of the sum of negative frequencies?
![[Pasted image 20240112234452.png]]
![[Pasted image 20240112234521.png]]

# Feature Extraction with Frequencies

Given a corpus with positive and negative tweets as follows:
![[Pasted image 20240112234713.png]]

You have to encode each tweet as a vector. Previously, this vector was of dimension $V$. Now, as you will see in the upcoming videos, you will represent it with a vector of dimension $3$. To do so, you have to create a dictionary to map the word, and the class it appeared in (positive or negative) to the number of times that word appeared in its corresponding class.
![[Pasted image 20240112234825.png]]
In the past two videos, we call this dictionary `freqs`. In the table above, you can see how words like happy and sad tend to take clear sides, while other words like "I, am" tend to be more neutral. Given this dictionary and the tweet, "I am sad, I am not learning NLP", you can create a vector corresponding to the feature as follows:
![[Pasted image 20240112234905.png]]
To encode the negative feature, you can do the same thing.
![[Pasted image 20240112234920.png]]

Hence you end up getting the following feature vector $[1,8,11]$. $1$ corresponds to the bias, $8$ the positive feature, and $11$ the negative feature.

# Preprocessing

![[index_8.mp4]]

# Preprocessing
When preprocessing, you have to perform the following:

1. Eliminate handles and URLs
    
2. Tokenize the string into words.
    
3. Remove stop words like "and, is, a, on, etc."
    
4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.
    
5. Convert all your words to lower case.
    

For example the following tweet $"@YMourri and @AndrewYNg are tuning a GREAT AI model at https://deeplearning.ai !!!"$ after preprocessing becomes

![[Pasted image 20240112235430.png]]

$[tun,great,ai,model]$. Hence you can see how we eliminated handles, tokenized it into words, removed stop words, performed stemming, and converted everything to lower case.

